{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":6347913,"sourceType":"datasetVersion","datasetId":3655505}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from pyspark.sql import SparkSession","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T16:47:58.924011Z","iopub.execute_input":"2025-07-01T16:47:58.924410Z","iopub.status.idle":"2025-07-01T16:47:59.341746Z","shell.execute_reply.started":"2025-07-01T16:47:58.924383Z","shell.execute_reply":"2025-07-01T16:47:59.340729Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"Spark = SparkSession.builder.appName('One').getOrCreate()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T16:48:44.619259Z","iopub.execute_input":"2025-07-01T16:48:44.619603Z","iopub.status.idle":"2025-07-01T16:48:52.707232Z","shell.execute_reply.started":"2025-07-01T16:48:44.619576Z","shell.execute_reply":"2025-07-01T16:48:52.706200Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"Spark","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T16:49:09.559128Z","iopub.execute_input":"2025-07-01T16:49:09.559484Z","iopub.status.idle":"2025-07-01T16:49:11.241823Z","shell.execute_reply.started":"2025-07-01T16:49:09.559453Z","shell.execute_reply":"2025-07-01T16:49:11.240951Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dfSpark = Spark.read.csv('/kaggle/input/retail-sales-dataset/retail_sales_dataset.csv',header=True,inferSchema=True)\ndfSpark.printSchema()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T16:51:05.359513Z","iopub.execute_input":"2025-07-01T16:51:05.360497Z","iopub.status.idle":"2025-07-01T16:51:12.180976Z","shell.execute_reply.started":"2025-07-01T16:51:05.360463Z","shell.execute_reply":"2025-07-01T16:51:12.179868Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dfSpark.show(2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T16:52:47.274983Z","iopub.execute_input":"2025-07-01T16:52:47.275383Z","iopub.status.idle":"2025-07-01T16:52:47.770005Z","shell.execute_reply.started":"2025-07-01T16:52:47.275361Z","shell.execute_reply":"2025-07-01T16:52:47.768930Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"TotalCustomer = dfSpark.select('Customer ID').count()\ndisplay(TotalCustomer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T16:52:55.780803Z","iopub.execute_input":"2025-07-01T16:52:55.781131Z","iopub.status.idle":"2025-07-01T16:52:56.645590Z","shell.execute_reply.started":"2025-07-01T16:52:55.781102Z","shell.execute_reply":"2025-07-01T16:52:56.644602Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"UniqueCustomers = dfSpark.select('Customer ID').distinct()\nUniqueCustomers.count()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T16:53:03.439192Z","iopub.execute_input":"2025-07-01T16:53:03.439543Z","iopub.status.idle":"2025-07-01T16:53:04.624813Z","shell.execute_reply.started":"2025-07-01T16:53:03.439515Z","shell.execute_reply":"2025-07-01T16:53:04.623265Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"What distinct customer groups can we identify based on demographics and purchase behavior to better target marketing and sales efforts?","metadata":{}},{"cell_type":"code","source":"from pyspark.ml.feature import StringIndexer, VectorAssembler, StandardScaler\nfrom pyspark.ml.clustering import KMeans","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T16:53:12.148711Z","iopub.execute_input":"2025-07-01T16:53:12.149004Z","iopub.status.idle":"2025-07-01T16:53:12.609223Z","shell.execute_reply.started":"2025-07-01T16:53:12.148983Z","shell.execute_reply":"2025-07-01T16:53:12.608007Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Encode categorical variables\ngender_indexer = StringIndexer(inputCol=\"Gender\", outputCol=\"GenderIndex\")\ncategory_indexer = StringIndexer(inputCol=\"Product Category\", outputCol=\"CategoryIndex\")\n\ndf_indexed = gender_indexer.fit(dfSpark).transform(dfSpark)\ndf_indexed = category_indexer.fit(df_indexed).transform(df_indexed)\n\n# Assemble features\nassembler = VectorAssembler(\n    inputCols=[\"Age\", \"Quantity\", \"Total Amount\", \"GenderIndex\", \"CategoryIndex\"], outputCol=\"features\")\n\ndf_assembled = assembler.transform(df_indexed)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T16:53:21.703884Z","iopub.execute_input":"2025-07-01T16:53:21.704467Z","iopub.status.idle":"2025-07-01T16:53:23.360809Z","shell.execute_reply.started":"2025-07-01T16:53:21.704438Z","shell.execute_reply":"2025-07-01T16:53:23.359900Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Standardize\nscaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")\ndf_scaled = scaler.fit(df_assembled).transform(df_assembled)\n\n# KMeans Clustering\nkmeans = KMeans(featuresCol=\"scaledFeatures\", k=4, seed=42)\nmodel = kmeans.fit(df_scaled)\npredictions = model.transform(df_scaled)\n\npredictions.select(\"Customer ID\", \"prediction\").show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T16:53:33.721677Z","iopub.execute_input":"2025-07-01T16:53:33.721983Z","iopub.status.idle":"2025-07-01T16:53:40.319484Z","shell.execute_reply.started":"2025-07-01T16:53:33.721964Z","shell.execute_reply":"2025-07-01T16:53:40.318456Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Can we predict daily sales for each product category based on the previous dayâ€™s sales data to improve inventory and demand planning?","metadata":{}},{"cell_type":"code","source":"from pyspark.sql.functions import to_date, sum as _sum\n\n# Ensure Date is in proper format\ndf = dfSpark.withColumn(\"Date\", to_date(\"Date\", \"yyyy-MM-dd\"))\n\n# Aggregate sales by date and category\ndf_sales = df.groupBy(\"Date\", \"Product Category\") \\\n    .agg(_sum(\"Total Amount\").alias(\"DailySales\"))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T16:54:14.463495Z","iopub.execute_input":"2025-07-01T16:54:14.463793Z","iopub.status.idle":"2025-07-01T16:54:14.531066Z","shell.execute_reply.started":"2025-07-01T16:54:14.463773Z","shell.execute_reply":"2025-07-01T16:54:14.529996Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from pyspark.sql.window import Window\nfrom pyspark.sql.functions import lag\n\n# Create lag feature for previous day sales\nwindowSpec = Window.partitionBy(\"Product Category\").orderBy(\"Date\")\ndf_sales = df_sales.withColumn(\"PrevDaySales\", lag(\"DailySales\", 1).over(windowSpec))\n\n# Drop nulls (from lag)\ndf_lagged = df_sales.dropna()\n\n# Assemble features\nassembler = VectorAssembler(inputCols=[\"PrevDaySales\"], outputCol=\"features\")\ndf_features = assembler.transform(df_lagged)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T16:54:21.958523Z","iopub.execute_input":"2025-07-01T16:54:21.958837Z","iopub.status.idle":"2025-07-01T16:54:22.092239Z","shell.execute_reply.started":"2025-07-01T16:54:21.958814Z","shell.execute_reply":"2025-07-01T16:54:22.091128Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Train/Test Split\n(train, test) = df_features.randomSplit([0.8, 0.2], seed=42)\n\n# Regression Model\nfrom pyspark.ml.regression import LinearRegression\nlr = LinearRegression(featuresCol=\"features\", labelCol=\"DailySales\")\nmodel = lr.fit(train)\npredictions = model.transform(test)\n\npredictions.select(\"Date\", \"Product Category\", \"DailySales\", \"prediction\").show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T16:54:34.338714Z","iopub.execute_input":"2025-07-01T16:54:34.339147Z","iopub.status.idle":"2025-07-01T16:54:37.745730Z","shell.execute_reply.started":"2025-07-01T16:54:34.339121Z","shell.execute_reply":"2025-07-01T16:54:37.744951Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"How does the price per unit affect the quantity sold, and can we predict sales volume based on product pricing?","metadata":{}},{"cell_type":"code","source":"from pyspark.ml.regression import LinearRegression\n\n# Assemble features\nassembler = VectorAssembler(inputCols=[\"Price per Unit\"], outputCol=\"features\")\ndf_reg = assembler.transform(df)\n\n# Regression Model: Quantity ~ PricePerUnit\nlr = LinearRegression(featuresCol=\"features\", labelCol=\"Quantity\")\nmodel = lr.fit(df_reg)\npredictions = model.transform(df_reg)\n\npredictions.select(\"Product Category\", \"Price per Unit\", \"Quantity\", \"prediction\").show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T16:55:00.338990Z","iopub.execute_input":"2025-07-01T16:55:00.339372Z","iopub.status.idle":"2025-07-01T16:55:01.055900Z","shell.execute_reply.started":"2025-07-01T16:55:00.339334Z","shell.execute_reply":"2025-07-01T16:55:01.054825Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from pyspark.sql.functions import collect_set\n\nbasket_df = dfSpark.groupBy(\"Transaction ID\") \\\n    .agg(collect_set(\"Product Category\").alias(\"items\"))\n\nbasket_df.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T16:55:12.739100Z","iopub.execute_input":"2025-07-01T16:55:12.739521Z","iopub.status.idle":"2025-07-01T16:55:13.204871Z","shell.execute_reply.started":"2025-07-01T16:55:12.739496Z","shell.execute_reply":"2025-07-01T16:55:13.202454Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from pyspark.ml.fpm import FPGrowth\n\n# Run FPGrowth\nfpGrowth = FPGrowth(itemsCol=\"items\", minSupport=0.01, minConfidence=0.3)\nmodel = fpGrowth.fit(basket_df)\n\nmodel.freqItemsets.show(truncate=False)\nmodel.associationRules.show(truncate=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T16:55:20.748546Z","iopub.execute_input":"2025-07-01T16:55:20.748886Z","iopub.status.idle":"2025-07-01T16:55:21.962161Z","shell.execute_reply.started":"2025-07-01T16:55:20.748863Z","shell.execute_reply":"2025-07-01T16:55:21.961315Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from pyspark.sql.functions import size\n\nbasket_df = df.groupBy(\"Transaction ID\") \\\n    .agg(collect_set(\"Product Category\").alias(\"items\"))\n\n# Check how many transactions have more than 1 item\nbasket_df.withColumn(\"num_items\", size(\"items\")).groupBy(\"num_items\").count().show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T16:55:29.598825Z","iopub.execute_input":"2025-07-01T16:55:29.599189Z","iopub.status.idle":"2025-07-01T16:55:30.102413Z","shell.execute_reply.started":"2025-07-01T16:55:29.599162Z","shell.execute_reply":"2025-07-01T16:55:30.101114Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from pyspark.sql.functions import rand\n\n# Randomly assign a few customers multiple categories (just for experimentation)\ndf_fake = df.sample(withReplacement=True, fraction=0.3).withColumn(\"Transaction ID\", df[\"Transaction ID\"] + rand())\n\n# Combine with original\ndf_combined = df.unionByName(df_fake)\n\n# Recreate basket_df\nbasket_df = df_combined.groupBy(\"Transaction ID\") \\\n    .agg(collect_set(\"Product Category\").alias(\"items\"))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T16:55:38.624771Z","iopub.execute_input":"2025-07-01T16:55:38.625125Z","iopub.status.idle":"2025-07-01T16:55:38.770680Z","shell.execute_reply.started":"2025-07-01T16:55:38.625098Z","shell.execute_reply":"2025-07-01T16:55:38.767408Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from pyspark.ml.fpm import FPGrowth\n\nfpGrowth = FPGrowth(itemsCol=\"items\", minSupport=0.01, minConfidence=0.3)\nmodel = fpGrowth.fit(basket_df)\n\nmodel.freqItemsets.show(truncate=False)\nmodel.associationRules.show(truncate=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T16:55:44.878724Z","iopub.execute_input":"2025-07-01T16:55:44.879063Z","iopub.status.idle":"2025-07-01T16:55:45.995422Z","shell.execute_reply.started":"2025-07-01T16:55:44.879022Z","shell.execute_reply":"2025-07-01T16:55:45.994598Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Can we accurately predict the product category a customer will buy based on their demographics and transaction details?","metadata":{}},{"cell_type":"code","source":"from pyspark.ml.classification import RandomForestClassifier\n\n# Encode categorical label\ncategory_indexer = StringIndexer(inputCol=\"Product Category\", outputCol=\"label\")\ngender_indexer = StringIndexer(inputCol=\"Gender\", outputCol=\"GenderIndex\")\n\ndf_encoded = gender_indexer.fit(df).transform(df)\ndf_encoded = category_indexer.fit(df_encoded).transform(df_encoded)\n\n# Assemble features\nassembler = VectorAssembler(inputCols=[\"Age\", \"GenderIndex\", \"Quantity\", \"Total Amount\"], outputCol=\"features\")\ndf_features = assembler.transform(df_encoded)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T16:55:56.149568Z","iopub.execute_input":"2025-07-01T16:55:56.149892Z","iopub.status.idle":"2025-07-01T16:55:56.902884Z","shell.execute_reply.started":"2025-07-01T16:55:56.149869Z","shell.execute_reply":"2025-07-01T16:55:56.901800Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Train/test split\n(train, test) = df_features.randomSplit([0.8, 0.2], seed=42)\n\n# Random Forest\nrf = RandomForestClassifier(featuresCol=\"features\", labelCol=\"label\", numTrees=50)\nmodel = rf.fit(train)\npredictions = model.transform(test)\n\npredictions.select(\"Age\", \"Gender\", \"Quantity\", \"Total Amount\", \"label\", \"prediction\").show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T16:57:23.003853Z","iopub.execute_input":"2025-07-01T16:57:23.005362Z","iopub.status.idle":"2025-07-01T16:57:26.352947Z","shell.execute_reply.started":"2025-07-01T16:57:23.005311Z","shell.execute_reply":"2025-07-01T16:57:26.352002Z"}},"outputs":[],"execution_count":null}]}